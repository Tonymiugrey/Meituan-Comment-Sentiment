{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4A731693A8584B6D9DD631A211408F5D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 大众点评评价情感分析~\n",
    "先上结果：\n",
    "\n",
    "| 糖水店的评论文本                             | 模型预测的情感评分 |\n",
    "| :------------------------------------------- | :----------------- |\n",
    "| '糖水味道不错，滑而不腻，赞一个，下次还会来' | 0.91               |\n",
    "| '味道一般，没啥特点'                         | 0.52               |\n",
    "| '排队老半天，环境很差，味道一般般'           | 0.05               |\n",
    "\n",
    "模型的效果还可以的样子，yeah~接下来我们好好讲讲怎么做的哈，我们通过爬虫爬取了大众点评广州8家最热门糖水店的3W条评论信息以及评分作为训练数据，前面的分析我们得知*样本很不均衡*。接下来我们的整体思路就是：文本特征处理(分词、去停用词、TF-IDF)—机器学习建模—模型评价。\n",
    "\n",
    "我们先不处理样本不均衡问题，直接建模后查看结果，接下来我们再按照两种方法处理样本不均衡，对比结果。\n",
    "\n",
    "### 数据读入和探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T10:10:40.773210Z",
     "start_time": "2023-03-17T10:10:36.645293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: eli5 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (1.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (1.16.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (0.20.1)\n",
      "Requirement already satisfied: attrs>17.1.0 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (21.4.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (1.21.5)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (3.1.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from eli5) (0.8.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from jinja2>=3.0.0->eli5) (2.0.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\miugrey\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20->eli5) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T10:10:51.643944Z",
     "start_time": "2023-03-17T10:10:47.330915Z"
    },
    "id": "6D9C349217E74A2A92566C7491B45B53",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import jieba\n",
    "import eli5\n",
    "train_data = pd.read_csv('bert/Meituan/train.csv')\n",
    "val_data = pd.read_csv('bert/Meituan/dev.csv')\n",
    "test_data = pd.read_csv('bert/Meituan/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B4B22D2FCE44B63847ED706482C6435",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 构建标签值\n",
    "\n",
    "大众点评的评分分为1-5分，1-2为差评，4-5为好评，3为中评，因此我们把1-2记为0,4-5记为1,3为中评，对我们的情感分析作用不大，丢弃掉这部分数据，但是可以作为训练语料模型的语料。我们的情感评分可以转化为标签值为1的概率值，这样我们就把情感分析问题转为文本分类问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T10:12:25.729371Z",
     "start_time": "2023-03-17T10:12:25.680355Z"
    },
    "id": "3097CE59B9A047ADAFDEB9FEC295FB7C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#构建label值\n",
    "def zhuanhuan(score):\n",
    "    if score > 3:\n",
    "        return 1\n",
    "    elif score < 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "#特征值转换\n",
    "train_data['target'] = train_data['star'].map(lambda x:zhuanhuan(x))\n",
    "train_data = train_data.dropna()\n",
    "val_data['target'] = val_data['star'].map(lambda x:zhuanhuan(x))\n",
    "val_data = val_data.dropna()\n",
    "test_data['target'] = test_data['star'].map(lambda x:zhuanhuan(x))\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "x_train = train_data[\"review\"]\n",
    "y_train = train_data[\"target\"]\n",
    "\n",
    "x_val = val_data[\"review\"]\n",
    "y_val = val_data[\"target\"]\n",
    "\n",
    "x_test = test_data[\"review\"]\n",
    "y_test = test_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BA9DD0B059E4BFB8F98EA369C0FC047",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 文本特征处理\n",
    "\n",
    "中文文本特征处理，需要进行中文分词，jieba分词库简单好用。接下来需要过滤停用词，网上能够搜到现成的。最后就要进行文本转向量，有词库表示法、TF-IDF、word2vec等，这篇文章作了详细介绍，推荐一波 https://zhuanlan.zhihu.com/p/44917421\n",
    "\n",
    "这里我们使用sklearn库的TF-IDF工具进行文本特征提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T10:13:07.765683Z",
     "start_time": "2023-03-17T10:12:29.998355Z"
    },
    "id": "541AEFEFC38146218A459ED2CE3F1608",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#引入停用词\n",
    "infile = open(\"bert/stopwords-zh.txt\",encoding='utf-8')\n",
    "stopwords_lst = infile.readlines()\n",
    "stopwords = [x.strip() for x in stopwords_lst]\n",
    "\n",
    "#中文分词\n",
    "def fenci(train_data):\n",
    "    words_df = train_data.apply(lambda x:' '.join(jieba.cut(x)))\n",
    "    return words_df\n",
    "\n",
    "x_train_fenci = fenci(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T10:13:10.637613Z",
     "start_time": "2023-03-17T10:13:08.006489Z"
    },
    "id": "2F1E0F545A3944AC8958AF4456CB41E1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(lowercase=False, max_features=5000,\n",
       "                stop_words=[&#x27;、&#x27;, &#x27;。&#x27;, &#x27;〈&#x27;, &#x27;〉&#x27;, &#x27;《&#x27;, &#x27;》&#x27;, &#x27;一&#x27;, &#x27;一个&#x27;, &#x27;一些&#x27;, &#x27;一何&#x27;,\n",
       "                            &#x27;一切&#x27;, &#x27;一则&#x27;, &#x27;一方面&#x27;, &#x27;一旦&#x27;, &#x27;一来&#x27;, &#x27;一样&#x27;, &#x27;一种&#x27;, &#x27;一般&#x27;,\n",
       "                            &#x27;一转眼&#x27;, &#x27;七&#x27;, &#x27;万一&#x27;, &#x27;三&#x27;, &#x27;上&#x27;, &#x27;上下&#x27;, &#x27;下&#x27;, &#x27;不&#x27;, &#x27;不仅&#x27;,\n",
       "                            &#x27;不但&#x27;, &#x27;不光&#x27;, &#x27;不单&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False, max_features=5000,\n",
       "                stop_words=[&#x27;、&#x27;, &#x27;。&#x27;, &#x27;〈&#x27;, &#x27;〉&#x27;, &#x27;《&#x27;, &#x27;》&#x27;, &#x27;一&#x27;, &#x27;一个&#x27;, &#x27;一些&#x27;, &#x27;一何&#x27;,\n",
       "                            &#x27;一切&#x27;, &#x27;一则&#x27;, &#x27;一方面&#x27;, &#x27;一旦&#x27;, &#x27;一来&#x27;, &#x27;一样&#x27;, &#x27;一种&#x27;, &#x27;一般&#x27;,\n",
       "                            &#x27;一转眼&#x27;, &#x27;七&#x27;, &#x27;万一&#x27;, &#x27;三&#x27;, &#x27;上&#x27;, &#x27;上下&#x27;, &#x27;下&#x27;, &#x27;不&#x27;, &#x27;不仅&#x27;,\n",
       "                            &#x27;不但&#x27;, &#x27;不光&#x27;, &#x27;不单&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(lowercase=False, max_features=5000,\n",
       "                stop_words=['、', '。', '〈', '〉', '《', '》', '一', '一个', '一些', '一何',\n",
       "                            '一切', '一则', '一方面', '一旦', '一来', '一样', '一种', '一般',\n",
       "                            '一转眼', '七', '万一', '三', '上', '上下', '下', '不', '不仅',\n",
       "                            '不但', '不光', '不单', ...])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#使用tf-idf把文本转为向量\n",
    "tv = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase = False)\n",
    "tv.fit(x_train_fenci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCCBAC788FD64A4A82FC62BBE4D4827E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 机器学习建模\n",
    "\n",
    "特征和标签已经准备好了，接下来就是建模了。这里我们使用文本分类的经典算法朴素贝叶斯算法，而且朴素贝叶斯算法的计算量较少。特征值是评论文本经过TF-IDF处理的向量，标签值评论的分类共两类，好评是1，差评是0。情感评分为分类器预测分类1的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:32.221Z"
    },
    "id": "4C5AC4A6FE564648813008A1286F6E0A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lr = LogisticRegression(max_iter=10000)\n",
    "model_lr.fit(tv.transform(fenci(x_train)), y_train)\n",
    "\n",
    "model_lr.score(tv.transform(fenci(x_val)), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:33.139Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred_lr = model_lr.predict(tv.transform(fenci(x_test)))\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:33.442Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:33.666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:33.892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test, y_pred_lr, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:34.167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, y_pred_lr, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:35.256Z"
    }
   },
   "outputs": [],
   "source": [
    "# F1\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_pred_lr, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:35.550Z"
    },
    "id": "142FE62F654A40948E94D459E8116FBB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#计算一条评论文本的情感评分\n",
    "def ceshi(model,strings):\n",
    "    strings_fenci = fenci(pd.Series([strings]))\n",
    "    return float(model.predict_proba(tv.transform(strings_fenci))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:12:36.869Z"
    },
    "id": "A7B74C0D6EE44D2E89507BE1FA716ED5",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#从大众点评网找两条评论来测试一下\n",
    "test1 = '很好吃，环境好，所有员工的态度都很好，上菜快，服务也很好，味道好吃，都是用蒸馏水煮的，推荐，超好吃' #5星好评\n",
    "test2 = '糯米外皮不绵滑，豆沙馅粗躁，没有香甜味。12元一碗不值。' #1星差评\n",
    "print('好评实例的模型预测情感得分为{}\\n差评实例的模型预测情感得分为{}'.format(ceshi(model_lr,test1),ceshi(model_lr,test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A8EBAB23F15482BB194C0DA6894D426",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看出，准确率和AUC值都非常不错的样子，但点评网上的实际测试中，5星好评模型预测出来了，1星差评缺预测错误。为什么呢？我们查看一下**混淆矩阵**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4373F1CDF80F463588742F08CD66E474",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看出，**负类的预测非常不准**，433单准确预测为负类的只有15.7%，应该是由于**数据不平衡**导致的，模型的默认阈值为输出值的中位数。比如逻辑回归的输出范围为[0,1]，当某个样本的输出大于0.5就会被划分为正例，反之为反例。在数据的类别不平衡时，采用默认的分类阈值可能会导致输出全部为正例，产生虚假的高准确度，导致分类失败。\n",
    "\n",
    "处理样本不均衡问题的方法，首先可以选择调整阈值，使得模型对于较少的类别更为敏感，或者选择合适的评估标准，比如ROC或者F1，而不是准确度（accuracy）。另外一种方法就是通过采样（sampling）来调整数据的不平衡。其中欠采样抛弃了大部分正例数据，从而弱化了其影响，可能会造成偏差很大的模型，同时，数据总是宝贵的，抛弃数据是很奢侈的。另外一种是过采样，下面我们就使用过采样方法来调整。\n",
    "\n",
    "### 过采样（单纯复制）\n",
    "\n",
    "单纯的重复了反例，因此会过分强调已有的反例。如果其中部分点标记错误或者是噪音，那么错误也容易被成倍的放大。因此最大的风险就是对反例过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-17T10:13:10.000Z"
    },
    "id": "18F3BD73DA0146C899D85503667D3554",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "33B8970B720E4448827CE0B627A50867",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  49,  382],\n",
       "       [   8, 4984]])"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_predict = classifier.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "79D8742F1B134C9281C0F76B9BAB880C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#把0类样本复制10次，构造训练集\n",
    "index_tmp = y_train==0\n",
    "y_tmp = y_train[index_tmp]\n",
    "x_tmp = x_train[index_tmp]\n",
    "x_train2 = pd.concat([x_train,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp,x_tmp])\n",
    "y_train2 = pd.concat([y_train,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp,y_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "D1111D2DFC0B458D8394A1368E76BA09",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8937064965197216"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用过采样样本(简单复制)进行模型训练，并查看准确率\n",
    "clf2 = MultinomialNB()\n",
    "clf2.fit(tv.transform(x_train2), y_train2)\n",
    "y_pred2 = clf2.predict_proba(tv.transform(x_test))[:,1]\n",
    "roc_auc_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "91C82C77D9044BBC8A54B8DE804F37F7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 330,  101],\n",
       "       [ 691, 4301]])"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看此时的混淆矩阵\n",
    "y_predict2 = clf2.predict(tv.transform(x_test))\n",
    "cm = confusion_matrix(y_test, y_predict2)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E1637FED7154AF09CCF5F7CA7E8CB48",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看出，即使是简单粗暴的复制样本来处理样本不平衡问题，负样本的识别率大幅上升了，变为77%，满满的幸福感呀~我们自己写两句评语来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BD36EA9CDD3B47E289B4465170AEEC8B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2821197346200878"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceshi(clf2,'排队人太多，环境不好，口味一般')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A150A8BBF9E4F3888F3687BA3F1701A",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看出把0类别的识别出来了，太棒了~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "388575F29C844A7D87C926091854960B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 后续优化方向\n",
    "\n",
    "- 使用更复杂的机器学习模型如神经网络、支持向量机等\n",
    "- 模型的调参\n",
    "- 行业词库的构建\n",
    "- 增加数据量\n",
    "- 优化情感分析的算法\n",
    "- 增加标签提取等\n",
    "- 项目部署到服务器上，更好地分享和测试模型的效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh-cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
